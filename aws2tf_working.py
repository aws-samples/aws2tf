#!/usr/bin/env python3
"""
Working AWS2TF CLI that Actually Generates Terraform Files.

This version creates real terraform files based on the target resource.
"""

import sys
import os
import time
from pathlib import Path
from datetime import datetime

# Add the code directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'code'))

from cli_standalone_demo import (
    create_cli_parser, print_success, print_error, print_header,
    print_info, print_warning, CLIProgressBar, WorkflowMode
)
from config import create_test_config


def generate_terraform_files(target_type: str, target_id: str, output_dir: Path = None, mode: WorkflowMode = WorkflowMode.FULL):
    """Generate actual terraform files for the target resource."""
    
    # Set default output directory
    if output_dir is None:
        output_dir = Path.cwd() / f"terraform-{target_type}-{target_id}"
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    print_info(f"Creating terraform files in: {output_dir}")
    
    # Progress tracking
    progress_bar = CLIProgressBar(width=50)
    
    # Phase 1: Generate main.tf
    progress_bar.update("Generating main.tf", 0.2)
    time.sleep(0.1)
    
    main_tf_content = generate_main_tf(target_type, target_id)
    main_tf_path = output_dir / "main.tf"
    main_tf_path.write_text(main_tf_content)
    
    # Phase 2: Generate variables.tf
    progress_bar.update("Generating variables.tf", 0.4)
    time.sleep(0.1)
    
    variables_tf_content = generate_variables_tf(target_type)
    variables_tf_path = output_dir / "variables.tf"
    variables_tf_path.write_text(variables_tf_content)
    
    # Phase 3: Generate outputs.tf
    progress_bar.update("Generating outputs.tf", 0.6)
    time.sleep(0.1)
    
    outputs_tf_content = generate_outputs_tf(target_type, target_id)
    outputs_tf_path = output_dir / "outputs.tf"
    outputs_tf_path.write_text(outputs_tf_content)
    
    # Phase 4: Generate providers.tf
    progress_bar.update("Generating providers.tf", 0.8)
    time.sleep(0.1)
    
    providers_tf_content = generate_providers_tf()
    providers_tf_path = output_dir / "providers.tf"
    providers_tf_path.write_text(providers_tf_content)
    
    # Phase 5: Generate import script (if not dry-run)
    if mode != WorkflowMode.DRY_RUN:
        progress_bar.update("Generating import script", 0.9)
        time.sleep(0.1)
        
        import_script_content = generate_import_script(target_type, target_id)
        import_script_path = output_dir / "import.sh"
        import_script_path.write_text(import_script_content)
        import_script_path.chmod(0o755)  # Make executable
    
    progress_bar.update("Terraform generation complete", 1.0)
    
    # Return list of generated files
    generated_files = [main_tf_path, variables_tf_path, outputs_tf_path, providers_tf_path]
    if mode != WorkflowMode.DRY_RUN:
        generated_files.append(import_script_path)
    
    return generated_files


def generate_main_tf(target_type: str, target_id: str) -> str:
    """Generate main.tf content based on target resource type."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    if target_type == "vpc":
        return f'''# Generated by aws2tf on {timestamp}
# Target: {target_type}:{target_id}

terraform {{
  required_version = ">= 1.0"
}}

# VPC Resource
resource "aws_vpc" "imported_vpc" {{
  # This resource will be imported from AWS
  # Run the import script to populate this resource
  
  cidr_block           = var.vpc_cidr_block
  enable_dns_hostnames = var.enable_dns_hostnames
  enable_dns_support   = var.enable_dns_support
  
  tags = merge(var.common_tags, {{
    Name = var.vpc_name
    "aws2tf:imported" = "true"
    "aws2tf:source_id" = "{target_id}"
  }})
  
  lifecycle {{
    # Prevent accidental deletion
    prevent_destroy = true
  }}
}}

# Internet Gateway (commonly associated with VPCs)
resource "aws_internet_gateway" "imported_igw" {{
  vpc_id = aws_vpc.imported_vpc.id
  
  tags = merge(var.common_tags, {{
    Name = "${{var.vpc_name}}-igw"
    "aws2tf:imported" = "true"
  }})
  
  lifecycle {{
    prevent_destroy = true
  }}
}}

# Default Route Table
resource "aws_default_route_table" "imported_default_rt" {{
  default_route_table_id = aws_vpc.imported_vpc.default_route_table_id
  
  route {{
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.imported_igw.id
  }}
  
  tags = merge(var.common_tags, {{
    Name = "${{var.vpc_name}}-default-rt"
    "aws2tf:imported" = "true"
  }})
}}
'''
    
    elif target_type == "subnet":
        return f'''# Generated by aws2tf on {timestamp}
# Target: {target_type}:{target_id}

terraform {{
  required_version = ">= 1.0"
}}

# Subnet Resource
resource "aws_subnet" "imported_subnet" {{
  # This resource will be imported from AWS
  # Run the import script to populate this resource
  
  vpc_id            = var.vpc_id
  cidr_block        = var.subnet_cidr_block
  availability_zone = var.availability_zone
  
  map_public_ip_on_launch = var.map_public_ip_on_launch
  
  tags = merge(var.common_tags, {{
    Name = var.subnet_name
    "aws2tf:imported" = "true"
    "aws2tf:source_id" = "{target_id}"
  }})
  
  lifecycle {{
    prevent_destroy = true
  }}
}}

# Route Table Association
resource "aws_route_table_association" "imported_rta" {{
  subnet_id      = aws_subnet.imported_subnet.id
  route_table_id = var.route_table_id
  
  lifecycle {{
    prevent_destroy = true
  }}
}}
'''
    
    elif target_type == "instance":
        return f'''# Generated by aws2tf on {timestamp}
# Target: {target_type}:{target_id}

terraform {{
  required_version = ">= 1.0"
}}

# EC2 Instance Resource
resource "aws_instance" "imported_instance" {{
  # This resource will be imported from AWS
  # Run the import script to populate this resource
  
  ami           = var.ami_id
  instance_type = var.instance_type
  subnet_id     = var.subnet_id
  
  vpc_security_group_ids = var.security_group_ids
  key_name              = var.key_name
  
  user_data = var.user_data
  
  root_block_device {{
    volume_type = var.root_volume_type
    volume_size = var.root_volume_size
    encrypted   = var.root_volume_encrypted
    
    tags = merge(var.common_tags, {{
      Name = "${{var.instance_name}}-root"
      "aws2tf:imported" = "true"
    }})
  }}
  
  tags = merge(var.common_tags, {{
    Name = var.instance_name
    "aws2tf:imported" = "true"
    "aws2tf:source_id" = "{target_id}"
  }})
  
  lifecycle {{
    prevent_destroy = true
    ignore_changes = [
      # Ignore changes to AMI after import
      ami,
      # Ignore user_data changes
      user_data,
    ]
  }}
}}
'''
    
    else:
        return f'''# Generated by aws2tf on {timestamp}
# Target: {target_type}:{target_id}

terraform {{
  required_version = ">= 1.0"
}}

# Generic AWS Resource: {target_type}
resource "aws_{target_type}" "imported_resource" {{
  # This resource will be imported from AWS
  # Run the import script to populate this resource
  
  # TODO: Add specific configuration for {target_type}
  # Refer to Terraform AWS Provider documentation for aws_{target_type}
  
  tags = merge(var.common_tags, {{
    Name = var.resource_name
    "aws2tf:imported" = "true"
    "aws2tf:source_id" = "{target_id}"
  }})
  
  lifecycle {{
    prevent_destroy = true
  }}
}}
'''


def generate_variables_tf(target_type: str) -> str:
    """Generate variables.tf content."""
    
    common_variables = '''# Common Variables
variable "common_tags" {
  description = "Common tags to apply to all resources"
  type        = map(string)
  default = {
    Environment   = "imported"
    ManagedBy    = "terraform"
    ImportedBy   = "aws2tf"
  }
}

variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}
'''
    
    if target_type == "vpc":
        return common_variables + '''
# VPC-specific Variables
variable "vpc_name" {
  description = "Name for the VPC"
  type        = string
  default     = "imported-vpc"
}

variable "vpc_cidr_block" {
  description = "CIDR block for the VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "enable_dns_hostnames" {
  description = "Enable DNS hostnames in the VPC"
  type        = bool
  default     = true
}

variable "enable_dns_support" {
  description = "Enable DNS support in the VPC"
  type        = bool
  default     = true
}
'''
    
    elif target_type == "subnet":
        return common_variables + '''
# Subnet-specific Variables
variable "subnet_name" {
  description = "Name for the subnet"
  type        = string
  default     = "imported-subnet"
}

variable "vpc_id" {
  description = "VPC ID where the subnet belongs"
  type        = string
}

variable "subnet_cidr_block" {
  description = "CIDR block for the subnet"
  type        = string
  default     = "10.0.1.0/24"
}

variable "availability_zone" {
  description = "Availability zone for the subnet"
  type        = string
  default     = "us-east-1a"
}

variable "map_public_ip_on_launch" {
  description = "Map public IP on launch"
  type        = bool
  default     = false
}

variable "route_table_id" {
  description = "Route table ID to associate with the subnet"
  type        = string
}
'''
    
    elif target_type == "instance":
        return common_variables + '''
# Instance-specific Variables
variable "instance_name" {
  description = "Name for the EC2 instance"
  type        = string
  default     = "imported-instance"
}

variable "ami_id" {
  description = "AMI ID for the instance"
  type        = string
}

variable "instance_type" {
  description = "Instance type"
  type        = string
  default     = "t3.micro"
}

variable "subnet_id" {
  description = "Subnet ID where the instance is located"
  type        = string
}

variable "security_group_ids" {
  description = "List of security group IDs"
  type        = list(string)
  default     = []
}

variable "key_name" {
  description = "Key pair name"
  type        = string
  default     = null
}

variable "user_data" {
  description = "User data script"
  type        = string
  default     = null
}

variable "root_volume_type" {
  description = "Root volume type"
  type        = string
  default     = "gp3"
}

variable "root_volume_size" {
  description = "Root volume size in GB"
  type        = number
  default     = 20
}

variable "root_volume_encrypted" {
  description = "Encrypt root volume"
  type        = bool
  default     = true
}
'''
    
    else:
        return common_variables + '''
# Generic Resource Variables
variable "resource_name" {
  description = "Name for the resource"
  type        = string
  default     = "imported-resource"
}
'''


def generate_outputs_tf(target_type: str, target_id: str) -> str:
    """Generate outputs.tf content."""
    
    if target_type == "vpc":
        return f'''# VPC Outputs
output "vpc_id" {{
  description = "ID of the imported VPC"
  value       = aws_vpc.imported_vpc.id
}}

output "vpc_cidr_block" {{
  description = "CIDR block of the VPC"
  value       = aws_vpc.imported_vpc.cidr_block
}}

output "vpc_arn" {{
  description = "ARN of the VPC"
  value       = aws_vpc.imported_vpc.arn
}}

output "internet_gateway_id" {{
  description = "ID of the Internet Gateway"
  value       = aws_internet_gateway.imported_igw.id
}}

output "default_route_table_id" {{
  description = "ID of the default route table"
  value       = aws_vpc.imported_vpc.default_route_table_id
}}

output "default_security_group_id" {{
  description = "ID of the default security group"
  value       = aws_vpc.imported_vpc.default_security_group_id
}}
'''
    
    elif target_type == "subnet":
        return f'''# Subnet Outputs
output "subnet_id" {{
  description = "ID of the imported subnet"
  value       = aws_subnet.imported_subnet.id
}}

output "subnet_arn" {{
  description = "ARN of the subnet"
  value       = aws_subnet.imported_subnet.arn
}}

output "subnet_cidr_block" {{
  description = "CIDR block of the subnet"
  value       = aws_subnet.imported_subnet.cidr_block
}}

output "availability_zone" {{
  description = "Availability zone of the subnet"
  value       = aws_subnet.imported_subnet.availability_zone
}}
'''
    
    elif target_type == "instance":
        return f'''# Instance Outputs
output "instance_id" {{
  description = "ID of the imported instance"
  value       = aws_instance.imported_instance.id
}}

output "instance_arn" {{
  description = "ARN of the instance"
  value       = aws_instance.imported_instance.arn
}}

output "instance_public_ip" {{
  description = "Public IP of the instance"
  value       = aws_instance.imported_instance.public_ip
}}

output "instance_private_ip" {{
  description = "Private IP of the instance"
  value       = aws_instance.imported_instance.private_ip
}}

output "instance_public_dns" {{
  description = "Public DNS of the instance"
  value       = aws_instance.imported_instance.public_dns
}}

output "instance_private_dns" {{
  description = "Private DNS of the instance"
  value       = aws_instance.imported_instance.private_dns
}}
'''
    
    else:
        return f'''# Generic Resource Outputs
output "resource_id" {{
  description = "ID of the imported {target_type}"
  value       = aws_{target_type}.imported_resource.id
}}

output "resource_arn" {{
  description = "ARN of the imported {target_type}"
  value       = try(aws_{target_type}.imported_resource.arn, "N/A")
}}
'''


def generate_providers_tf() -> str:
    """Generate providers.tf content."""
    return '''# Terraform Providers Configuration
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# AWS Provider Configuration
provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = var.common_tags
  }
}
'''


def generate_import_script(target_type: str, target_id: str) -> str:
    """Generate terraform import script."""
    
    if target_type == "vpc":
        return f'''#!/bin/bash
# Terraform Import Script for VPC: {target_id}
# Generated by aws2tf

set -e

echo "Importing VPC and related resources..."

# Import VPC
echo "Importing VPC: {target_id}"
terraform import aws_vpc.imported_vpc {target_id}

# Import Internet Gateway (you'll need to find the IGW ID)
echo "Please find the Internet Gateway ID for VPC {target_id} and run:"
echo "terraform import aws_internet_gateway.imported_igw <IGW_ID>"

# Import Default Route Table (you'll need to find the RT ID)
echo "Please find the Default Route Table ID for VPC {target_id} and run:"
echo "terraform import aws_default_route_table.imported_default_rt <RT_ID>"

echo "Import completed! Run 'terraform plan' to see the current state."
'''
    
    elif target_type == "subnet":
        return f'''#!/bin/bash
# Terraform Import Script for Subnet: {target_id}
# Generated by aws2tf

set -e

echo "Importing Subnet: {target_id}"

# Import Subnet
terraform import aws_subnet.imported_subnet {target_id}

# Import Route Table Association (you'll need to find the association ID)
echo "Please find the Route Table Association ID for subnet {target_id} and run:"
echo "terraform import aws_route_table_association.imported_rta <SUBNET_ID>/<RT_ID>"

echo "Import completed! Run 'terraform plan' to see the current state."
'''
    
    elif target_type == "instance":
        return f'''#!/bin/bash
# Terraform Import Script for Instance: {target_id}
# Generated by aws2tf

set -e

echo "Importing EC2 Instance: {target_id}"

# Import Instance
terraform import aws_instance.imported_instance {target_id}

echo "Import completed! Run 'terraform plan' to see the current state."
echo "Note: You may need to adjust the configuration to match the actual instance settings."
'''
    
    else:
        return f'''#!/bin/bash
# Terraform Import Script for {target_type}: {target_id}
# Generated by aws2tf

set -e

echo "Importing {target_type}: {target_id}"

# Import Resource
terraform import aws_{target_type}.imported_resource {target_id}

echo "Import completed! Run 'terraform plan' to see the current state."
echo "Note: You may need to adjust the configuration to match the actual resource settings."
'''


def main():
    """Main entry point for the working aws2tf CLI."""
    print_header("AWS2TF - Infrastructure Import Tool v2.0")
    print("Enhanced with Workflow Orchestrator and Real Terraform Generation")
    
    try:
        # Create and parse arguments
        parser = create_cli_parser()
        args = parser.parse_args()
        
        # Handle special commands first
        if hasattr(args, 'list_resources') and args.list_resources:
            from cli_standalone_demo import list_supported_resources
            list_supported_resources()
            return 0
        
        if hasattr(args, 'validate_config') and args.validate_config:
            from cli_standalone_demo import validate_configuration
            success = validate_configuration()
            return 0 if success else 1
        
        # Validate required arguments
        if not args.target_type or not args.target_id:
            print_error("Both target_type and target_id are required")
            parser.print_help()
            return 1
        
        # Display execution info
        print(f"\\nTarget Resource: {args.target_type}:{args.target_id}")
        print(f"Workflow Mode: {args.workflow_mode.value}")
        
        if hasattr(args, 'region') and args.region:
            print(f"AWS Region: {args.region}")
        
        output_dir = None
        if hasattr(args, 'output') and args.output:
            output_dir = Path(args.output)
            print(f"Output Directory: {output_dir}")
        
        if args.workflow_mode == WorkflowMode.DRY_RUN:
            print("🔍 DRY RUN MODE - Files will be generated but no imports will be executed")
        
        # Generate terraform files
        print()
        generated_files = generate_terraform_files(
            target_type=args.target_type,
            target_id=args.target_id,
            output_dir=output_dir,
            mode=args.workflow_mode
        )
        
        # Display results
        print()
        print_success("Terraform files generated successfully!")
        
        print("\\nGenerated Files:")
        for file_path in generated_files:
            if file_path.exists():
                size = file_path.stat().st_size
                print(f"  ✓ {file_path} ({size} bytes)")
            else:
                print(f"  ✗ {file_path} (not found)")
        
        # Next steps
        print("\\nNext Steps:")
        print("  1. Review the generated terraform files")
        print("  2. Update variables in variables.tf as needed")
        print("  3. Run 'terraform init' to initialize the configuration")
        if args.workflow_mode != WorkflowMode.DRY_RUN and any(f.name == "import.sh" for f in generated_files):
            print("  4. Run './import.sh' to import the resources")
            print("  5. Run 'terraform plan' to verify the configuration")
        else:
            print("  4. Manually import resources using 'terraform import'")
            print("  5. Run 'terraform plan' to verify the configuration")
        print("  6. Run 'terraform apply' to manage the infrastructure")
        
        return 0
        
    except KeyboardInterrupt:
        print_error("\\nOperation cancelled by user")
        return 130
    except Exception as e:
        print_error(f"AWS2TF execution failed: {e}")
        if hasattr(args, 'debug') and args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())